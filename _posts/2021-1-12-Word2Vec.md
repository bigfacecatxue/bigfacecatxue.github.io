# Word2Vec

## One-hot vector 
One-hot vector is the most simple way to represent words. Each vector combines 0s and 1 for index of the word. Like
[0,0,0.....1,0,0]. It's dense and inefficient. Because you can't get anything from this matrix.

## Co-occurrence
This method we count the number of times each word appears within a window around a center word, because if a word frequently appears with another word, it implies it's high probability they appear together. Words appear together will have a similar vector, when multiplied together, they have a higher probability than with other words. 

After counted this (V,V) matrix, we apply SVD to reduce v dimensions to k dimensions, each row vector is a word vector.

## Word2Vec
Co-occurrence is a good idea, but we have a better idea. That is maybe we can reduce v dimensions to a few hundreds dimensions to capture each words features. First we assume maybe 100 dimensions(practial 200/300/1000 are all options) for each word, and we train a model to update those parameters(|V||100| matrix). 

There are two algorithms: continuous bag-of-words(CBOW) and skip-gram. Skip-gram is picking a center word, and calculate the probability of words surround within a window. Like a sentence "I like deep learning", we take center word "like", within window size of 1, we calculate the probability of P(I **|** like) and P(deep **|** like), we use softmax to calculate the probability, we can try to maxmize the probability. Then we get an effient vector matrix to represent words.

Continuous bag-of-words is the opposite way to skip-gram. CBOW takes a context and predict the center word. In above exemple, we predict probability of P(like|I,deep).

And there are two training methods: negative sampling and hierarchical softmax, both avoid summation over V dimentions. Negative sampling is to maxmize the probability of a word and context being in the corpus data if it indeed is, and maxmize the probability of a word and context not being in the corpus data if it indeed is not. Hierarchical softmax uses a binary tree where leaves are the words, the probability of a word being the output word is the probability of a random walk from the root to that word's leaf.
