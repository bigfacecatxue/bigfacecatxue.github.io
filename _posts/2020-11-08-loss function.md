# Loss Function And Gradient Descent

To understand how neural network works, we must understand loss function and gradient descent. The reason why architecture can predicts targets correctly is because architecture captures the most important features of targets. Architecture is combined by parameters, the more accurate the parameters, the more accurate the model.

First, we initialize parameters randomly, then calculate predictions. At first predictions are not good, so we need to update parameters. Since we have predictions and targets, we can calculate the loss to measure how good the architecture performs. We just need to decrease the loss to achive a better architecture. Amazing thins is gradient descent can do this. The function is parameters = parameters - gradient descent * learning rate. Wonderful thing is that gradient descent leads the direction to decrease loss. If gradient descent is positive, parameter needs to decrease to move to minum loss, if gradient descent is negative, parameter needs to increase to move to minum loss. To speed up this process, we multiply gradient descent with a learning rate. With an appropriate learning rate, we can update parameter fast and accurate.

Another important thing is to pick up a loss function. A good loss function can measure how good architecure performs and can calculate a good gradient descent. Cross_entropy is pretty good at this. Softmax measures probabilities of all predictions, and pick up the highest probability prediction. Because all probabilities sum to 1, so we only need to max the target's prediction. Log the target's prediction, because all probabilites are between 0 and 1, so the log result are negative infinite to 0, we just need to negative log likelihood, it will be the loss of our prediction.

So loss function measures the difference between targets and predictions, gradient descent helps update paremeters to decrease loss, and stochastic gradient descent is to take a mini_batch data to calculate gradient descent, just to speed up the process. After a few epoches, luckly we can get a pretty good architecture. 
